{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "np.random.seed(2016)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import __version__ as keras_version\n",
    "import data_set_ooc\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = dict()\n",
    "FLAGS['width'] = 32\n",
    "FLAGS['height'] = 32\n",
    "FLAGS['kernel_1_out'] = 8\n",
    "FLAGS['kernel_2_out'] = 8\n",
    "FLAGS['conv2_input_width'] = 16\n",
    "FLAGS['conv2_input_height'] = 16\n",
    "FLAGS['n_classes'] = 8\n",
    "FLAGS['learning_rate'] = 0.001\n",
    "FLAGS['batch_size'] = 16\n",
    "FLAGS['n_epochs'] = 20\n",
    "FLAGS['train_report_step'] = 20\n",
    "FLAGS['val_report_step'] = 80\n",
    "FLAGS['keep_prob'] = 0.75\n",
    "FLAGS['reg'] = 0.01\n",
    "FLAGS['patience'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data_set_ooc.DataSet(width = FLAGS['width'], height=FLAGS['height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "tb = TensorBoard(log_dir='keras_tb/', histogram_freq=5, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(32, 32, 3)))\n",
    "    model.add(Convolution2D(4, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(4, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(8, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(8, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_on_val(data, model, batch_size=20):\n",
    "    data.reset_val_index()\n",
    "    n_batches = int((data.num_val_examples / batch_size))\n",
    "    val_loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        (X_batch, y_batch) = data.next_val_batch(batch_size) \n",
    "        \n",
    "        val_loss += model.test_on_batch(X_batch, y_batch)\n",
    "    return (val_loss / n_batches)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for 3776 steps\n",
      "loss for batch 19:1.7720377445220947\n",
      "loss for batch 39:1.6299201250076294\n",
      "loss for batch 59:1.9017510414123535\n",
      "loss for batch 79:1.6430813074111938\n",
      "loss for batch 99:1.4196301698684692\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 99:1.6248446474684046\n",
      "loss for batch 119:1.7226126194000244\n",
      "loss for batch 139:1.2384369373321533\n",
      "loss for batch 159:1.7265989780426025\n",
      "loss for batch 179:1.5586460828781128\n",
      "loss for batch 199:1.4797132015228271\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 199:1.611971066353169\n",
      "loss for batch 219:1.2945077419281006\n",
      "loss for batch 239:1.7211583852767944\n",
      "loss for batch 259:1.488649606704712\n",
      "loss for batch 279:1.8336536884307861\n",
      "loss for batch 299:1.3154619932174683\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 299:1.6031947693926223\n",
      "loss for batch 319:1.2942259311676025\n",
      "loss for batch 339:1.5841209888458252\n",
      "loss for batch 359:1.482114315032959\n",
      "loss for batch 379:1.5433952808380127\n",
      "loss for batch 399:1.4497491121292114\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 399:1.6009923650863322\n",
      "loss for batch 419:1.7414761781692505\n",
      "loss for batch 439:1.6053248643875122\n",
      "loss for batch 459:1.2994930744171143\n",
      "loss for batch 479:1.7470905780792236\n",
      "loss for batch 499:1.694214105606079\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 499:1.5959851513517664\n",
      "loss for batch 519:1.7423648834228516\n",
      "loss for batch 539:2.0748863220214844\n",
      "loss for batch 559:2.1097240447998047\n",
      "loss for batch 579:1.887267827987671\n",
      "loss for batch 599:1.725785493850708\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 599:1.57943435425454\n",
      "loss for batch 619:1.6492574214935303\n",
      "loss for batch 639:1.4106029272079468\n",
      "loss for batch 659:1.3302117586135864\n",
      "loss for batch 679:1.6344815492630005\n",
      "loss for batch 699:1.423162817955017\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 699:1.5597664346086217\n",
      "loss for batch 719:1.3857601881027222\n",
      "loss for batch 739:1.684017300605774\n",
      "loss for batch 759:1.967489242553711\n",
      "loss for batch 779:1.3701679706573486\n",
      "loss for batch 799:1.7124533653259277\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 799:1.5305608206606927\n",
      "loss for batch 819:1.3484628200531006\n",
      "loss for batch 839:1.7291127443313599\n",
      "loss for batch 859:1.6389654874801636\n",
      "loss for batch 879:1.3399296998977661\n",
      "loss for batch 899:1.4933903217315674\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 899:1.4866109959622646\n",
      "loss for batch 919:2.0601868629455566\n",
      "loss for batch 939:1.4047775268554688\n",
      "loss for batch 959:1.734374761581421\n",
      "loss for batch 979:1.5214643478393555\n",
      "loss for batch 999:1.1974296569824219\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 999:1.4435053657978139\n",
      "loss for batch 1019:1.2673914432525635\n",
      "loss for batch 1039:1.8599846363067627\n",
      "loss for batch 1059:1.4549033641815186\n",
      "loss for batch 1079:1.4877004623413086\n",
      "loss for batch 1099:1.5235391855239868\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 1099:1.377749190685597\n",
      "loss for batch 1119:1.7615689039230347\n",
      "loss for batch 1139:1.6812629699707031\n",
      "loss for batch 1159:1.4137775897979736\n",
      "loss for batch 1179:1.3691638708114624\n",
      "loss for batch 1199:1.311650276184082\n",
      "Evaluating on validation set\n",
      "Validation has not improved for 1 rounds\n",
      "validation loss for step 1199:1.391671227647903\n",
      "loss for batch 1219:1.3611173629760742\n",
      "loss for batch 1239:1.6736642122268677\n",
      "loss for batch 1259:1.388157606124878\n",
      "loss for batch 1279:1.536876916885376\n",
      "loss for batch 1299:1.5770647525787354\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 1299:1.3612014600571165\n",
      "loss for batch 1319:1.2602616548538208\n",
      "loss for batch 1339:1.614850401878357\n",
      "loss for batch 1359:1.2283591032028198\n",
      "loss for batch 1379:1.328780174255371\n",
      "loss for batch 1399:1.300654411315918\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 1399:1.3098995292440374\n",
      "loss for batch 1419:1.1059024333953857\n",
      "loss for batch 1439:1.423464059829712\n",
      "loss for batch 1459:1.5019415616989136\n",
      "loss for batch 1479:1.290024995803833\n",
      "loss for batch 1499:1.2269468307495117\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 1499:1.2541990191378491\n",
      "loss for batch 1519:1.0350632667541504\n",
      "loss for batch 1539:0.8889361619949341\n",
      "loss for batch 1559:1.6559194326400757\n",
      "loss for batch 1579:1.2329127788543701\n",
      "loss for batch 1599:1.1636404991149902\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 1599:1.226859799090852\n",
      "loss for batch 1619:1.4375150203704834\n",
      "loss for batch 1639:1.0738911628723145\n",
      "loss for batch 1659:0.8474880456924438\n",
      "loss for batch 1679:1.4753801822662354\n",
      "loss for batch 1699:1.208397388458252\n",
      "Evaluating on validation set\n",
      "Validation has not improved for 1 rounds\n",
      "validation loss for step 1699:1.2501491358939638\n",
      "loss for batch 1719:1.842910885810852\n",
      "loss for batch 1739:1.8053410053253174\n",
      "loss for batch 1759:1.1458340883255005\n",
      "loss for batch 1779:1.2995704412460327\n",
      "loss for batch 1799:1.1037780046463013\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 1799:1.199577476115937\n",
      "loss for batch 1819:1.263293981552124\n",
      "loss for batch 1839:1.098975419998169\n",
      "loss for batch 1859:1.2872074842453003\n",
      "loss for batch 1879:1.5191463232040405\n",
      "loss for batch 1899:1.3217463493347168\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 1899:1.1340299768650786\n",
      "loss for batch 1919:1.6113862991333008\n",
      "loss for batch 1939:1.0133131742477417\n",
      "loss for batch 1959:1.3352313041687012\n",
      "loss for batch 1979:1.580470085144043\n",
      "loss for batch 1999:1.0288326740264893\n",
      "Evaluating on validation set\n",
      "Validation has not improved for 1 rounds\n",
      "validation loss for step 1999:1.1524748726094023\n",
      "loss for batch 2019:1.0504084825515747\n",
      "loss for batch 2039:1.2143750190734863\n",
      "loss for batch 2059:1.6469488143920898\n",
      "loss for batch 2079:0.9785388708114624\n",
      "loss for batch 2099:0.8674303293228149\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 2099:1.1049418633288526\n",
      "loss for batch 2119:1.330674648284912\n",
      "loss for batch 2139:1.110931158065796\n",
      "loss for batch 2159:0.8113312125205994\n",
      "loss for batch 2179:0.8137384653091431\n",
      "loss for batch 2199:1.1179721355438232\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 2199:1.0475433215181877\n",
      "loss for batch 2219:1.3020461797714233\n",
      "loss for batch 2239:1.2306151390075684\n",
      "loss for batch 2259:1.2804450988769531\n",
      "loss for batch 2279:1.2538297176361084\n",
      "loss for batch 2299:1.1771754026412964\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 2299:1.0343810421355226\n",
      "loss for batch 2319:0.8085422515869141\n",
      "loss for batch 2339:1.2171502113342285\n",
      "loss for batch 2359:0.9955360889434814\n",
      "loss for batch 2379:0.9703622460365295\n",
      "loss for batch 2399:1.2461678981781006\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 2399:0.990224835720468\n",
      "loss for batch 2419:1.1849331855773926\n",
      "loss for batch 2439:0.6814898252487183\n",
      "loss for batch 2459:0.8601825833320618\n",
      "loss for batch 2479:1.621492624282837\n",
      "loss for batch 2499:1.306279182434082\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 2499:0.9810575278515511\n",
      "loss for batch 2519:1.2439844608306885\n",
      "loss for batch 2539:0.8450818657875061\n",
      "loss for batch 2559:1.3951399326324463\n",
      "loss for batch 2579:0.7892035245895386\n",
      "loss for batch 2599:1.2416222095489502\n",
      "Evaluating on validation set\n",
      "Validation has not improved for 1 rounds\n",
      "validation loss for step 2599:0.9902601838111877\n",
      "loss for batch 2619:1.3277801275253296\n",
      "loss for batch 2639:1.5533370971679688\n",
      "loss for batch 2659:1.3525354862213135\n",
      "loss for batch 2679:1.4203366041183472\n",
      "loss for batch 2699:1.1557369232177734\n",
      "Evaluating on validation set\n",
      "Validation has not improved for 2 rounds\n",
      "validation loss for step 2699:0.9913016627443597\n",
      "loss for batch 2719:1.3501651287078857\n",
      "loss for batch 2739:0.968398928642273\n",
      "loss for batch 2759:0.9980345964431763\n",
      "loss for batch 2779:1.8663578033447266\n",
      "loss for batch 2799:1.4146525859832764\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 2799:0.9172768307493088\n",
      "loss for batch 2819:1.0396403074264526\n",
      "loss for batch 2839:0.6815297603607178\n",
      "loss for batch 2859:1.4957549571990967\n",
      "loss for batch 2879:1.039453148841858\n",
      "loss for batch 2899:1.245969295501709\n",
      "Evaluating on validation set\n",
      "Validation has not improved for 1 rounds\n",
      "validation loss for step 2899:0.9246811029758859\n",
      "loss for batch 2919:1.282080888748169\n",
      "loss for batch 2939:1.043729543685913\n",
      "loss for batch 2959:0.9670795202255249\n",
      "loss for batch 2979:1.1426109075546265\n",
      "loss for batch 2999:0.975829005241394\n",
      "Evaluating on validation set\n",
      "Validation has not improved for 2 rounds\n",
      "validation loss for step 2999:0.9252917779252884\n",
      "loss for batch 3019:0.74223792552948\n",
      "loss for batch 3039:0.9210644960403442\n",
      "loss for batch 3059:1.1285406351089478\n",
      "loss for batch 3079:1.2448221445083618\n",
      "loss for batch 3099:1.2486646175384521\n",
      "Evaluating on validation set\n",
      "Validation improved\n",
      "validation loss for step 3099:0.8705030362656776\n",
      "loss for batch 3119:1.043339490890503\n",
      "loss for batch 3139:1.3617331981658936\n",
      "loss for batch 3159:1.0221503973007202\n",
      "loss for batch 3179:1.1339995861053467\n",
      "loss for batch 3199:0.9344470500946045\n",
      "Evaluating on validation set\n",
      "Validation has not improved for 1 rounds\n",
      "validation loss for step 3199:0.8859625148012283\n",
      "loss for batch 3219:0.8680644631385803\n",
      "loss for batch 3239:1.410447120666504\n",
      "loss for batch 3259:1.2042083740234375\n",
      "loss for batch 3279:1.4058101177215576\n",
      "loss for batch 3299:0.5328042507171631\n",
      "Evaluating on validation set\n",
      "Validation has not improved for 2 rounds\n",
      "validation loss for step 3299:0.8919423602996989\n",
      "loss for batch 3319:0.509540319442749\n",
      "loss for batch 3339:1.1384060382843018\n",
      "loss for batch 3359:0.7769477367401123\n",
      "loss for batch 3379:1.4247453212738037\n",
      "loss for batch 3399:1.05021071434021\n",
      "Evaluating on validation set\n",
      "Validation has not improved for 3 rounds\n",
      "Validation loss has stopped improving. Stopping training\n"
     ]
    }
   ],
   "source": [
    "n_steps = int(FLAGS['n_epochs'] * (data.num_examples / FLAGS['batch_size']))\n",
    "print('training for {} steps'.format(n_steps))\n",
    "X_val, y_val = data.get_validation_set()\n",
    "best_val_loss = 20\n",
    "no_val_improvements = 0\n",
    "for step in range(n_steps):\n",
    "    X_batch, y_batch = data.next_batch(FLAGS['batch_size'])\n",
    "    batch_loss = model.train_on_batch(X_batch, y_batch)\n",
    "    if ((step + 1) % 20 == 0):\n",
    "        print('loss for batch {}:{}'.format(step, batch_loss))\n",
    "        \n",
    "    if ((step + 1) % 100 == 0):\n",
    "        print('Evaluating on validation set')\n",
    "        val_loss = eval_on_val(data, model, FLAGS['batch_size'])\n",
    "        if val_loss < best_val_loss:\n",
    "            print('Validation improved')\n",
    "            best_val_loss = val_loss\n",
    "            no_val_improvements = 0\n",
    "        else:\n",
    "            no_val_improvements +=1\n",
    "            print('Validation has not improved for {} rounds'.format(no_val_improvements))\n",
    "            if no_val_improvements == FLAGS['patience']:\n",
    "                print('Validation loss has stopped improving. Stopping training')\n",
    "                break\n",
    "        print('validation loss for step {}:{}'.format(step, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 0 of 1000\n",
      "loading 20 of 1000\n",
      "loading 40 of 1000\n",
      "loading 60 of 1000\n",
      "loading 80 of 1000\n",
      "loading 100 of 1000\n",
      "loading 120 of 1000\n",
      "loading 140 of 1000\n",
      "loading 160 of 1000\n",
      "loading 180 of 1000\n",
      "loading 200 of 1000\n",
      "loading 220 of 1000\n",
      "loading 240 of 1000\n",
      "loading 260 of 1000\n",
      "loading 280 of 1000\n",
      "loading 300 of 1000\n",
      "loading 320 of 1000\n",
      "loading 340 of 1000\n",
      "loading 360 of 1000\n",
      "loading 380 of 1000\n",
      "loading 400 of 1000\n",
      "loading 420 of 1000\n",
      "loading 440 of 1000\n",
      "loading 460 of 1000\n",
      "loading 480 of 1000\n",
      "loading 500 of 1000\n",
      "loading 520 of 1000\n",
      "loading 540 of 1000\n",
      "loading 560 of 1000\n",
      "loading 580 of 1000\n",
      "loading 600 of 1000\n",
      "loading 620 of 1000\n",
      "loading 640 of 1000\n",
      "loading 660 of 1000\n",
      "loading 680 of 1000\n",
      "loading 700 of 1000\n",
      "loading 720 of 1000\n",
      "loading 740 of 1000\n",
      "loading 760 of 1000\n",
      "loading 780 of 1000\n",
      "loading 800 of 1000\n",
      "loading 820 of 1000\n",
      "loading 840 of 1000\n",
      "loading 860 of 1000\n",
      "loading 880 of 1000\n",
      "loading 900 of 1000\n",
      "loading 920 of 1000\n",
      "loading 940 of 1000\n",
      "loading 960 of 1000\n",
      "loading 980 of 1000\n"
     ]
    }
   ],
   "source": [
    "import load_images_ooc\n",
    "X_test, y_test = load_images_ooc.load_test(width = FLAGS['width'], \n",
    "                                                      height=FLAGS['height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import make_submission\n",
    "submit = make_submission.makeSubmission(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALB</th>\n",
       "      <th>BET</th>\n",
       "      <th>DOL</th>\n",
       "      <th>LAG</th>\n",
       "      <th>NoF</th>\n",
       "      <th>OTHER</th>\n",
       "      <th>SHARK</th>\n",
       "      <th>YFT</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.480731</td>\n",
       "      <td>0.091640</td>\n",
       "      <td>0.038230</td>\n",
       "      <td>3.138173e-02</td>\n",
       "      <td>0.059931</td>\n",
       "      <td>0.104842</td>\n",
       "      <td>4.453060e-02</td>\n",
       "      <td>0.148713</td>\n",
       "      <td>img_07663.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>0.489360</td>\n",
       "      <td>0.091023</td>\n",
       "      <td>0.036115</td>\n",
       "      <td>3.233087e-02</td>\n",
       "      <td>0.060564</td>\n",
       "      <td>0.104608</td>\n",
       "      <td>4.333372e-02</td>\n",
       "      <td>0.142666</td>\n",
       "      <td>img_07678.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>0.425442</td>\n",
       "      <td>0.054473</td>\n",
       "      <td>0.021002</td>\n",
       "      <td>1.168269e-02</td>\n",
       "      <td>0.338403</td>\n",
       "      <td>0.026709</td>\n",
       "      <td>4.236940e-03</td>\n",
       "      <td>0.118051</td>\n",
       "      <td>img_07689.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>0.487482</td>\n",
       "      <td>0.091121</td>\n",
       "      <td>0.036313</td>\n",
       "      <td>3.263553e-02</td>\n",
       "      <td>0.060848</td>\n",
       "      <td>0.104780</td>\n",
       "      <td>4.390377e-02</td>\n",
       "      <td>0.142918</td>\n",
       "      <td>img_07700.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>0.861521</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>6.886035e-03</td>\n",
       "      <td>0.038153</td>\n",
       "      <td>0.024969</td>\n",
       "      <td>1.936493e-03</td>\n",
       "      <td>0.041788</td>\n",
       "      <td>img_07717.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>0.544640</td>\n",
       "      <td>0.070526</td>\n",
       "      <td>0.026507</td>\n",
       "      <td>3.604038e-02</td>\n",
       "      <td>0.034945</td>\n",
       "      <td>0.155779</td>\n",
       "      <td>1.828543e-02</td>\n",
       "      <td>0.113277</td>\n",
       "      <td>img_07746.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>0.921015</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>2.029999e-04</td>\n",
       "      <td>0.062197</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>6.619100e-05</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>img_07757.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>0.266553</td>\n",
       "      <td>0.097018</td>\n",
       "      <td>0.095287</td>\n",
       "      <td>1.118319e-02</td>\n",
       "      <td>0.040244</td>\n",
       "      <td>0.120980</td>\n",
       "      <td>2.957791e-02</td>\n",
       "      <td>0.339157</td>\n",
       "      <td>img_07792.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>0.675522</td>\n",
       "      <td>0.055076</td>\n",
       "      <td>0.014112</td>\n",
       "      <td>1.870452e-02</td>\n",
       "      <td>0.065001</td>\n",
       "      <td>0.059667</td>\n",
       "      <td>1.585725e-02</td>\n",
       "      <td>0.096060</td>\n",
       "      <td>img_07799.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>0.511695</td>\n",
       "      <td>0.032804</td>\n",
       "      <td>0.015799</td>\n",
       "      <td>5.942514e-03</td>\n",
       "      <td>0.289963</td>\n",
       "      <td>0.016157</td>\n",
       "      <td>2.782125e-03</td>\n",
       "      <td>0.124856</td>\n",
       "      <td>img_07818.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>0.769400</td>\n",
       "      <td>0.036582</td>\n",
       "      <td>0.007596</td>\n",
       "      <td>1.845510e-02</td>\n",
       "      <td>0.033562</td>\n",
       "      <td>0.076580</td>\n",
       "      <td>4.520365e-03</td>\n",
       "      <td>0.053304</td>\n",
       "      <td>img_07833.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>8.557478e-08</td>\n",
       "      <td>0.992713</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>7.433504e-08</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>img_07859.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>0.865387</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>9.608618e-04</td>\n",
       "      <td>0.093909</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>3.600820e-04</td>\n",
       "      <td>0.029117</td>\n",
       "      <td>img_07872.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>0.353233</td>\n",
       "      <td>0.094068</td>\n",
       "      <td>0.067231</td>\n",
       "      <td>1.365937e-02</td>\n",
       "      <td>0.030446</td>\n",
       "      <td>0.137046</td>\n",
       "      <td>1.463547e-02</td>\n",
       "      <td>0.289682</td>\n",
       "      <td>img_07893.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0.958864</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>3.901345e-04</td>\n",
       "      <td>0.026523</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>4.073398e-05</td>\n",
       "      <td>0.009431</td>\n",
       "      <td>img_07895.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.030389</td>\n",
       "      <td>0.007427</td>\n",
       "      <td>2.131220e-02</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>0.113258</td>\n",
       "      <td>2.885688e-03</td>\n",
       "      <td>0.047620</td>\n",
       "      <td>img_07905.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.318651</td>\n",
       "      <td>0.088095</td>\n",
       "      <td>0.095528</td>\n",
       "      <td>1.549772e-02</td>\n",
       "      <td>0.128455</td>\n",
       "      <td>0.067022</td>\n",
       "      <td>7.576592e-03</td>\n",
       "      <td>0.279175</td>\n",
       "      <td>img_07906.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.669599</td>\n",
       "      <td>0.042839</td>\n",
       "      <td>0.011075</td>\n",
       "      <td>3.639180e-02</td>\n",
       "      <td>0.022625</td>\n",
       "      <td>0.163406</td>\n",
       "      <td>4.527694e-03</td>\n",
       "      <td>0.049537</td>\n",
       "      <td>img_07908.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.966614</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>4.212205e-04</td>\n",
       "      <td>0.020430</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>3.837519e-05</td>\n",
       "      <td>0.007879</td>\n",
       "      <td>img_07910.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.453444</td>\n",
       "      <td>0.076210</td>\n",
       "      <td>0.041251</td>\n",
       "      <td>2.198259e-02</td>\n",
       "      <td>0.041659</td>\n",
       "      <td>0.148593</td>\n",
       "      <td>1.854414e-02</td>\n",
       "      <td>0.198317</td>\n",
       "      <td>img_07921.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ALB       BET       DOL           LAG       NoF     OTHER  \\\n",
       "980  0.480731  0.091640  0.038230  3.138173e-02  0.059931  0.104842   \n",
       "981  0.489360  0.091023  0.036115  3.233087e-02  0.060564  0.104608   \n",
       "982  0.425442  0.054473  0.021002  1.168269e-02  0.338403  0.026709   \n",
       "983  0.487482  0.091121  0.036313  3.263553e-02  0.060848  0.104780   \n",
       "984  0.861521  0.021484  0.003262  6.886035e-03  0.038153  0.024969   \n",
       "985  0.544640  0.070526  0.026507  3.604038e-02  0.034945  0.155779   \n",
       "986  0.921015  0.002207  0.000235  2.029999e-04  0.062197  0.000736   \n",
       "987  0.266553  0.097018  0.095287  1.118319e-02  0.040244  0.120980   \n",
       "988  0.675522  0.055076  0.014112  1.870452e-02  0.065001  0.059667   \n",
       "989  0.511695  0.032804  0.015799  5.942514e-03  0.289963  0.016157   \n",
       "990  0.769400  0.036582  0.007596  1.845510e-02  0.033562  0.076580   \n",
       "991  0.001867  0.000087  0.000023  8.557478e-08  0.992713  0.000002   \n",
       "992  0.865387  0.006674  0.001245  9.608618e-04  0.093909  0.002348   \n",
       "993  0.353233  0.094068  0.067231  1.365937e-02  0.030446  0.137046   \n",
       "994  0.958864  0.002261  0.000169  3.901345e-04  0.026523  0.002321   \n",
       "995  0.752484  0.030389  0.007427  2.131220e-02  0.024624  0.113258   \n",
       "996  0.318651  0.088095  0.095528  1.549772e-02  0.128455  0.067022   \n",
       "997  0.669599  0.042839  0.011075  3.639180e-02  0.022625  0.163406   \n",
       "998  0.966614  0.002218  0.000143  4.212205e-04  0.020430  0.002256   \n",
       "999  0.453444  0.076210  0.041251  2.198259e-02  0.041659  0.148593   \n",
       "\n",
       "            SHARK       YFT          image  \n",
       "980  4.453060e-02  0.148713  img_07663.jpg  \n",
       "981  4.333372e-02  0.142666  img_07678.jpg  \n",
       "982  4.236940e-03  0.118051  img_07689.jpg  \n",
       "983  4.390377e-02  0.142918  img_07700.jpg  \n",
       "984  1.936493e-03  0.041788  img_07717.jpg  \n",
       "985  1.828543e-02  0.113277  img_07746.jpg  \n",
       "986  6.619100e-05  0.013341  img_07757.jpg  \n",
       "987  2.957791e-02  0.339157  img_07792.jpg  \n",
       "988  1.585725e-02  0.096060  img_07799.jpg  \n",
       "989  2.782125e-03  0.124856  img_07818.jpg  \n",
       "990  4.520365e-03  0.053304  img_07833.jpg  \n",
       "991  7.433504e-08  0.005307  img_07859.jpg  \n",
       "992  3.600820e-04  0.029117  img_07872.jpg  \n",
       "993  1.463547e-02  0.289682  img_07893.jpg  \n",
       "994  4.073398e-05  0.009431  img_07895.jpg  \n",
       "995  2.885688e-03  0.047620  img_07905.jpg  \n",
       "996  7.576592e-03  0.279175  img_07906.jpg  \n",
       "997  4.527694e-03  0.049537  img_07908.jpg  \n",
       "998  3.837519e-05  0.007879  img_07910.jpg  \n",
       "999  1.854414e-02  0.198317  img_07921.jpg  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('keras_test_ooc_3399_steps.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3021 samples, validate on 756 samples\n",
      "INFO:tensorflow:Summary name convolution2d_1_W:0 is illegal; using convolution2d_1_W_0 instead.\n",
      "INFO:tensorflow:Summary name convolution2d_1_b:0 is illegal; using convolution2d_1_b_0 instead.\n",
      "INFO:tensorflow:Summary name convolution2d_2_W:0 is illegal; using convolution2d_2_W_0 instead.\n",
      "INFO:tensorflow:Summary name convolution2d_2_b:0 is illegal; using convolution2d_2_b_0 instead.\n",
      "INFO:tensorflow:Summary name convolution2d_3_W:0 is illegal; using convolution2d_3_W_0 instead.\n",
      "INFO:tensorflow:Summary name convolution2d_3_b:0 is illegal; using convolution2d_3_b_0 instead.\n",
      "INFO:tensorflow:Summary name convolution2d_4_W:0 is illegal; using convolution2d_4_W_0 instead.\n",
      "INFO:tensorflow:Summary name convolution2d_4_b:0 is illegal; using convolution2d_4_b_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1_W:0 is illegal; using dense_1_W_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1_b:0 is illegal; using dense_1_b_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2_W:0 is illegal; using dense_2_W_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2_b:0 is illegal; using dense_2_b_0 instead.\n",
      "INFO:tensorflow:Summary name dense_3_W:0 is illegal; using dense_3_W_0 instead.\n",
      "INFO:tensorflow:Summary name dense_3_b:0 is illegal; using dense_3_b_0 instead.\n",
      "Epoch 1/30\n",
      "16s - loss: 1.6622 - val_loss: 1.5902\n",
      "Epoch 2/30\n",
      "10s - loss: 1.5961 - val_loss: 1.5859\n",
      "Epoch 3/30\n",
      "9s - loss: 1.5898 - val_loss: 1.5848\n",
      "Epoch 4/30\n",
      "9s - loss: 1.5885 - val_loss: 1.5836\n",
      "Epoch 5/30\n",
      "10s - loss: 1.5825 - val_loss: 1.5845\n",
      "Epoch 6/30\n",
      "11s - loss: 1.5846 - val_loss: 1.5829\n",
      "Epoch 7/30\n",
      "12s - loss: 1.5827 - val_loss: 1.5840\n",
      "Epoch 8/30\n",
      "9s - loss: 1.5830 - val_loss: 1.5838\n",
      "Epoch 9/30\n",
      "8s - loss: 1.5825 - val_loss: 1.5826\n",
      "Epoch 10/30\n",
      "10s - loss: 1.5817 - val_loss: 1.5832\n",
      "Epoch 11/30\n",
      "15s - loss: 1.5823 - val_loss: 1.5833\n",
      "Epoch 12/30\n",
      "12s - loss: 1.5814 - val_loss: 1.5833\n",
      "Epoch 13/30\n",
      "10s - loss: 1.5820 - val_loss: 1.5837\n",
      "Epoch 14/30\n",
      "11s - loss: 1.5820 - val_loss: 1.5831\n",
      "Epoch 15/30\n",
      "9s - loss: 1.5802 - val_loss: 1.5823\n",
      "Epoch 16/30\n",
      "12s - loss: 1.5815 - val_loss: 1.5824\n",
      "Epoch 17/30\n",
      "10s - loss: 1.5812 - val_loss: 1.5823\n",
      "Epoch 18/30\n",
      "10s - loss: 1.5813 - val_loss: 1.5823\n",
      "Epoch 19/30\n",
      "9s - loss: 1.5812 - val_loss: 1.5825\n",
      "Epoch 20/30\n",
      "10s - loss: 1.5814 - val_loss: 1.5825\n",
      "Epoch 21/30\n",
      "13s - loss: 1.5808 - val_loss: 1.5830\n"
     ]
    }
   ],
   "source": [
    "callbacks a= [\n",
    "            EarlyStopping(monitor='val_loss', patience=5, verbose=0),\n",
    "tb]\n",
    "hist = model.fit(data.X_train, data.y_train, batch_size=FLAGS['batch_size'], nb_epoch=FLAGS['epochs'],\n",
    "          shuffle=True, verbose=2, validation_data=(X_val, y_val),\n",
    "              callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 0 of 1000\n",
      "loading 20 of 1000\n",
      "loading 40 of 1000\n",
      "loading 60 of 1000\n",
      "loading 80 of 1000\n",
      "loading 100 of 1000\n",
      "loading 120 of 1000\n",
      "loading 140 of 1000\n",
      "loading 160 of 1000\n",
      "loading 180 of 1000\n",
      "loading 200 of 1000\n",
      "loading 220 of 1000\n",
      "loading 240 of 1000\n",
      "loading 260 of 1000\n",
      "loading 280 of 1000\n",
      "loading 300 of 1000\n",
      "loading 320 of 1000\n",
      "loading 340 of 1000\n",
      "loading 360 of 1000\n",
      "loading 380 of 1000\n",
      "loading 400 of 1000\n",
      "loading 420 of 1000\n",
      "loading 440 of 1000\n",
      "loading 460 of 1000\n",
      "loading 480 of 1000\n",
      "loading 500 of 1000\n",
      "loading 520 of 1000\n",
      "loading 540 of 1000\n",
      "loading 560 of 1000\n",
      "loading 580 of 1000\n",
      "loading 600 of 1000\n",
      "loading 620 of 1000\n",
      "loading 640 of 1000\n",
      "loading 660 of 1000\n",
      "loading 680 of 1000\n",
      "loading 700 of 1000\n",
      "loading 720 of 1000\n",
      "loading 740 of 1000\n",
      "loading 760 of 1000\n",
      "loading 780 of 1000\n",
      "loading 800 of 1000\n",
      "loading 820 of 1000\n",
      "loading 840 of 1000\n",
      "loading 860 of 1000\n",
      "loading 880 of 1000\n",
      "loading 900 of 1000\n",
      "loading 920 of 1000\n",
      "loading 940 of 1000\n",
      "loading 960 of 1000\n",
      "loading 980 of 1000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test, batch_size=FLAGS['batch_size'], verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import make_submission\n",
    "submit = make_submission.makeSubmission(preds,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit.to_csv('keras_32_32.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
