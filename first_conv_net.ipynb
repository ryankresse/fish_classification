{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import load_images\n",
    "import data_set\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = dict()\n",
    "FLAGS['width'] = 160\n",
    "FLAGS['height'] = 90\n",
    "FLAGS['batch_size'] = 16\n",
    "FLAGS['kernel_1_out'] = 64\n",
    "FLAGS['kernel_2_out'] = 32\n",
    "FLAGS['conv2_input_width'] = 16\n",
    "FLAGS['conv2_input_height'] = 16\n",
    "FLAGS['n_classes'] = 8\n",
    "FLAGS['learning_rate'] = 0.001\n",
    "FLAGS['batch_size'] = 16\n",
    "FLAGS['n_epochs'] = 1\n",
    "FLAGS['train_report_step'] = 1\n",
    "FLAGS['val_report_step'] = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder ALB (Index: 0)\n",
      "Load folder BET (Index: 1)\n",
      "Load folder DOL (Index: 2)\n",
      "Load folder LAG (Index: 3)\n",
      "Load folder NoF (Index: 4)\n",
      "Load folder OTHER (Index: 5)\n",
      "Load folder SHARK (Index: 6)\n",
      "Load folder YFT (Index: 7)\n",
      "Read train data time: 318.85 seconds\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_train_id = load_images.load_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_train_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data_set.DataSet(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_train\n",
    "del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss for batch: -2.1586735248565674\n",
      "log loss for batch: 0.0\n",
      "log loss for batch: -6.476020812988281\n",
      "log loss for batch: -6.476020812988281\n",
      "log loss for batch: -10.793367385864258\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    with tf.name_scope('data'):\n",
    "        X = tf.placeholder(tf.float32, [None, None, None, 3], name=\"X_placeholder\")\n",
    "        Y = tf.placeholder(tf.float32, [None, FLAGS['n_classes']], name=\"Y_placeholder\")\n",
    "        #dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "        \n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        scope.reuse_variables()\n",
    "        kernel = tf.Variable(tf.truncated_normal(shape=[FLAGS['width'], \n",
    "                                                        FLAGS['height'], 3, FLAGS['kernel_1_out']], \n",
    "                                                        name='kernel'))\n",
    "        biases = tf.Variable(tf.random_normal(shape=[FLAGS['kernel_1_out']]), name=\"bias\")\n",
    "        conv = tf.nn.conv2d(X, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv1 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "        kernel1_summ = tf.summary.scalar('kernel1_mean', tf.reduce_mean(kernel))\n",
    "        conv1_summ = tf.summary.scalar('conv1_mean', tf.reduce_mean(conv1))\n",
    "        # output is of dimension 16 x HEIGHT x WIDTH x KERNEL_1_OUT\n",
    "   \n",
    "    with tf.variable_scope('pool1') as scope:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                padding='SAME')\n",
    "    \n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = tf.Variable(\n",
    "            tf.truncated_normal(shape=[FLAGS['conv2_input_width'], \n",
    "                                       FLAGS['conv2_input_height'], \n",
    "                                       FLAGS['kernel_1_out'], \n",
    "                                       FLAGS['kernel_2_out']], name='kernel'))\n",
    "\n",
    "        biases = tf.Variable(tf.random_normal(shape=[FLAGS['kernel_2_out']]), name=\"bias\")\n",
    "        conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "        kernel2_summ = tf.summary.scalar('kernel2_mean', tf.reduce_mean(kernel))\n",
    "        conv2_summ = tf.summary.scalar('conv2_mean', tf.reduce_mean(conv2))\n",
    "\n",
    "    with tf.variable_scope('pool2') as scope:\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                padding='SAME') \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    with tf.variable_scope('fc') as scope:\n",
    "\n",
    "        # reshape pool2 to 2 dimensional\n",
    "        pool2 = tf.reshape(pool2, [FLAGS['batch_size'], -1])\n",
    "        \n",
    "        #7680 is shape of pool2 second axis. \n",
    "        w = tf.Variable(tf.truncated_normal(shape=[7680, 1024], name='kernel'))\n",
    "        b = tf.Variable(tf.random_normal(shape=[1024]), name=\"bias\")\n",
    "        \n",
    "        fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "        fcw_summ = tf.summary.scalar('fcw_mean', tf.reduce_mean(w))\n",
    "        fc_summ = tf.summary.scalar('fc', tf.reduce_mean(fc))\n",
    "        #Ouput is (BATCH_SIZE, 1024)\n",
    "        #fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n",
    "    \n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        w = tf.Variable(tf.truncated_normal(shape=[1024, FLAGS['n_classes']],name='weights'))\n",
    "        softw_summ = tf.summary.scalar('softw_mean', tf.reduce_mean(w))\n",
    "        b = tf.Variable(tf.random_normal(shape=[FLAGS['n_classes']], name='biases'))\n",
    "        logits = tf.matmul(fc, w) + b\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "        loss = tf.reduce_mean(entropy, name='loss')\n",
    "\n",
    "        entropy_clipped = tf.clip_by_value(entropy, 1e-15, 1 - 1e-15)\n",
    "        log_loss = tf.convert_to_tensor(tf.reduce_mean(tf.log(entropy_clipped)))\n",
    "\n",
    "        loss_summ = tf.summary.scalar('loss', loss)\n",
    "        log_loss_summ = tf.summary.scalar('log_loss', log_loss)\n",
    "\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(FLAGS['learning_rate']).minimize(loss, \n",
    "                                            global_step=global_step)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "\n",
    "    with tf.Session(graph=g) as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        sess.run(init)\n",
    "\n",
    "        #saver = tf.train.Saver()\n",
    "        # to visualize using TensorBoard\n",
    "        #writer = tf.summary.FileWriter('./my_graph/mnist', sess.graph)\n",
    "        #ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_mnist_new/checkpoint'))\n",
    "        # if that checkpoint exists, restore from checkpoint\n",
    "        #if ckpt and ckpt.model_checkpoint_path:\n",
    "         #   saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        initial_step = global_step.eval()\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        n_batches = data.num_examples / FLAGS['batch_size']\n",
    "\n",
    "        total_loss = 0.0\n",
    "        train_writer = tf.summary.FileWriter('test-summary',\n",
    "                                          sess.graph)\n",
    "        val_writer = tf.summary.FileWriter('val-summary',\n",
    "                                          sess.graph)\n",
    "        train_writer.flush()\n",
    "        val_writer.flush()\n",
    "        X_val, y_val = data.get_validation_set()\n",
    "        \n",
    "        for index in range(initial_step, int(n_batches * FLAGS['n_epochs'])): # train the model n_epochs times\n",
    "            \n",
    "            X_batch, y_batch = data.next_batch(FLAGS['batch_size'])\n",
    "            \n",
    "            if (index + 1) % FLAGS['train_report_step'] == 0:\n",
    "                _, loss_batch, l_loss, summary = sess.run([optimizer, loss, log_loss, merged], \n",
    "                                    feed_dict={X: X_batch, Y:y_batch}) \n",
    "                total_loss += loss_batch\n",
    "                #print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / FLAGS['train_report_step']))\n",
    "                print('log loss for batch: {}'.format(l_loss))\n",
    "                train_writer.add_summary(summary, index)\n",
    "            else:\n",
    "                _, loss_batch = sess.run([optimizer, loss], \n",
    "                                    feed_dict={X: X_batch, Y:y_batch}) \n",
    "                total_loss += loss_batch\n",
    "            \n",
    "            if (index + 1) % FLAGS['val_report_step'] == 0:\n",
    "                \n",
    "                l_loss, summary  = sess.run([log_loss, merged], \n",
    "                                    feed_dict={X: X_val, Y:y_val}) \n",
    "                print('validation log loss for batch: {}'.format(l_loss))\n",
    "                val_writer.add_summary(summary, index)\n",
    "\n",
    "        #saver.save(sess, 'checkpoints/convnet_mnist_new/mnist-convnet', index)\n",
    "        #print(sess.run([loss], feed_dict={X:X_train[:10, :,:,:], Y:y_mod[:10]}))\n",
    "        #n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
