{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import load_images\n",
    "import data_set\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = dict()\n",
    "FLAGS['width'] = 32\n",
    "FLAGS['height'] = 32\n",
    "FLAGS['batch_size'] = 16\n",
    "FLAGS['kernel_1_out'] = 32\n",
    "FLAGS['kernel_2_out'] = 16\n",
    "FLAGS['conv2_input_width'] = 16\n",
    "FLAGS['conv2_input_height'] = 16\n",
    "FLAGS['n_classes'] = 8\n",
    "FLAGS['learning_rate'] = 0.001\n",
    "FLAGS['batch_size'] = 16\n",
    "FLAGS['n_epochs'] = 5\n",
    "FLAGS['train_report_step'] = 20\n",
    "FLAGS['val_report_step'] = 80\n",
    "FLAGS['keep_prob'] = 0.75\n",
    "FLAGS['reg'] = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder ALB (Index: 0)\n",
      "Load folder BET (Index: 1)\n",
      "Load folder DOL (Index: 2)\n",
      "Load folder LAG (Index: 3)\n",
      "Load folder NoF (Index: 4)\n",
      "Load folder OTHER (Index: 5)\n",
      "Load folder SHARK (Index: 6)\n",
      "Load folder YFT (Index: 7)\n",
      "Read train data time: 351.41 seconds\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_images.load_train(width = FLAGS['width'], \n",
    "                                                      height=FLAGS['height'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data_set.DataSet(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_train\n",
    "del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.add?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Average loss at step 20: 6854.5\n",
      "Average loss at step 40: 5054.6\n",
      "Average loss at step 60: 4223.8\n",
      "Average loss at step 80: 3658.1\n",
      "checking validation loss...\n",
      "validation loss: 98.42107411977408\n",
      "validation loss increased\n",
      "Average loss at step 100: 3274.5\n",
      "Average loss at step 120: 2998.0\n",
      "Average loss at step 140: 2785.5\n",
      "Average loss at step 160: 2618.2\n",
      "checking validation loss...\n",
      "validation loss: 83.79303258947424\n",
      "validation loss decreased...saving best params\n",
      "Average loss at step 180: 2477.4\n",
      "Average loss at step 200: 2359.8\n",
      "Average loss at step 220: 2257.9\n",
      "Average loss at step 240: 2169.4\n",
      "checking validation loss...\n",
      "validation loss: 74.48021520666174\n",
      "validation loss decreased...saving best params\n",
      "Average loss at step 260: 2091.9\n",
      "Average loss at step 280: 2023.7\n",
      "Average loss at step 300: 1963.4\n",
      "Average loss at step 320: 1909.6\n",
      "checking validation loss...\n",
      "validation loss: 69.58787990260768\n",
      "validation loss decreased...saving best params\n",
      "Average loss at step 340: 1861.3\n",
      "Average loss at step 360: 1817.7\n",
      "Average loss at step 380: 1777.9\n",
      "Average loss at step 400: 1741.4\n",
      "checking validation loss...\n",
      "validation loss: 66.23920683989654\n",
      "validation loss decreased...saving best params\n",
      "Average loss at step 420: 1707.9\n",
      "Average loss at step 440: 1676.9\n",
      "Average loss at step 460: 1648.0\n",
      "Average loss at step 480: 1621.1\n",
      "checking validation loss...\n",
      "validation loss: 63.312603429845865\n",
      "validation loss decreased...saving best params\n",
      "Average loss at step 500: 1595.9\n",
      "Average loss at step 520: 1572.3\n",
      "Average loss at step 540: 1550.0\n",
      "Average loss at step 560: 1529.0\n",
      "checking validation loss...\n",
      "validation loss: 60.756489521748314\n",
      "validation loss decreased...saving best params\n",
      "Average loss at step 580: 1509.1\n",
      "Average loss at step 600: 1490.2\n",
      "Average loss at step 620: 1472.3\n",
      "Average loss at step 640: 1455.2\n",
      "checking validation loss...\n",
      "validation loss: 58.47052620552682\n",
      "validation loss decreased...saving best params\n",
      "Average loss at step 660: 1438.8\n",
      "Average loss at step 680: 1423.2\n",
      "Average loss at step 700: 1408.3\n",
      "Average loss at step 720: 1394.0\n",
      "checking validation loss...\n",
      "validation loss: 56.47223708694045\n",
      "validation loss decreased...saving best params\n",
      "Average loss at step 740: 1380.2\n",
      "Average loss at step 760: 1367.0\n",
      "Average loss at step 780: 1354.3\n",
      "Average loss at step 800: 1342.1\n",
      "checking validation loss...\n",
      "validation loss: 54.69604401459565\n",
      "validation loss decreased...saving best params\n",
      "Average loss at step 820: 1330.2\n",
      "Average loss at step 840: 1318.8\n",
      "Average loss at step 860: 1307.8\n",
      "Average loss at step 880: 1297.2\n",
      "checking validation loss...\n",
      "validation loss: 53.11381423537796\n",
      "validation loss decreased...saving best params\n",
      "Average loss at step 900: 1286.9\n",
      "Average loss at step 920: 1276.9\n",
      "Average loss at step 940: 1267.2\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    with tf.name_scope('data'):\n",
    "        X = tf.placeholder(tf.float32, [None, None, None, 3], name=\"X_placeholder\")\n",
    "        Y = tf.placeholder(tf.int32, [None], name=\"Y_placeholder\")\n",
    "        dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "        \n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        scope.reuse_variables()\n",
    "        kernel1 = tf.Variable(tf.truncated_normal(shape=[FLAGS['width'], \n",
    "                                                        FLAGS['height'], 3, FLAGS['kernel_1_out']], \n",
    "                                                        name='kernel'))\n",
    "        biases = tf.Variable(tf.random_normal(shape=[FLAGS['kernel_1_out']]), name=\"bias\")\n",
    "        conv = tf.nn.conv2d(X, kernel1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv1 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "        kernel1_summ = tf.summary.scalar('kernel1_mean', tf.reduce_mean(kernel1))\n",
    "        conv1_summ = tf.summary.scalar('conv1_mean', tf.reduce_mean(conv1))\n",
    "   \n",
    "    with tf.variable_scope('pool1') as scope:\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                padding='SAME')\n",
    "    \n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel2 = tf.Variable(\n",
    "            tf.truncated_normal(shape=[FLAGS['conv2_input_width'], \n",
    "                                       FLAGS['conv2_input_height'], \n",
    "                                       FLAGS['kernel_1_out'], \n",
    "                                       FLAGS['kernel_2_out']], name='kernel'))\n",
    "\n",
    "        biases = tf.Variable(tf.random_normal(shape=[FLAGS['kernel_2_out']]), name=\"bias\")\n",
    "        conv = tf.nn.conv2d(pool1, kernel2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "        kernel2_summ = tf.summary.scalar('kernel2_mean', tf.reduce_mean(kernel2))\n",
    "        conv2_summ = tf.summary.scalar('conv2_mean', tf.reduce_mean(conv2))\n",
    "\n",
    "    with tf.variable_scope('pool2') as scope:\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                padding='SAME') \n",
    "    \n",
    "    with tf.variable_scope('fc') as scope:\n",
    "\n",
    "        # reshape pool2 to 2 dimensional\n",
    "        pool2 = tf.reshape(pool2, [FLAGS['batch_size'], -1])\n",
    "        \n",
    "\n",
    "        fcw = tf.Variable(tf.truncated_normal(shape=[1024, 128], name='kernel'))\n",
    "        b = tf.Variable(tf.random_normal(shape=[128]), name=\"bias\")\n",
    "        \n",
    "        fc = tf.nn.relu(tf.matmul(pool2, fcw) + b, name='relu')\n",
    "        fcw_summ = tf.summary.scalar('fcw_mean', tf.reduce_mean(fcw))\n",
    "        fc_summ = tf.summary.scalar('fc', tf.reduce_mean(fc))\n",
    "        fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n",
    "    \n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        sfw = tf.Variable(tf.truncated_normal(shape=[128, FLAGS['n_classes']],name='weights'))\n",
    "        softw_summ = tf.summary.scalar('softw_mean', tf.reduce_mean(sfw))\n",
    "        b = tf.Variable(tf.random_normal(shape=[FLAGS['n_classes']], name='biases'))\n",
    "        logits = tf.matmul(fc, sfw) + b\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels = Y)\n",
    "        reg_los = 0\n",
    "        reg_loss = 0.5 * FLAGS['reg'] * tf.reduce_sum(tf.square(kernel1))\n",
    "        reg_loss += 0.5 * FLAGS['reg'] * tf.reduce_sum(tf.square(kernel2))\n",
    "        reg_loss += 0.5 * FLAGS['reg'] * tf.reduce_sum(tf.square(fcw))\n",
    "        reg_loss += 0.5 * FLAGS['reg'] * tf.reduce_sum(tf.square(sfw))\n",
    "        loss += reg_loss\n",
    "        loss_summ = tf.summary.scalar('loss', loss)\n",
    "             \n",
    "    optimizer = tf.train.AdamOptimizer(FLAGS['learning_rate']).minimize(loss, \n",
    "                                            global_step=global_step)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    print('hello')\n",
    "\n",
    "    with tf.Session(graph=g) as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        sess.run(init)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        #to visualize using TensorBoard\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('val_saver/'))\n",
    "        # if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        initial_step = global_step.eval()\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        n_batches = data.num_examples / FLAGS['batch_size']\n",
    "\n",
    "        total_loss = 0.0\n",
    "        last_val_loss = 0\n",
    "        X_val, y_val = data.get_validation_set()\n",
    "        val_patience = 1\n",
    "        num_val_increases = 0\n",
    "        train_writer = tf.summary.FileWriter('early_stopping_summary',\n",
    "                                          sess.graph)\n",
    "        train_writer.flush()\n",
    "        val_writer.flush()\n",
    "        for index in np.arange(initial_step, int(n_batches * FLAGS['n_epochs'])): # train the model n_epochs times\n",
    "            X_batch, y_batch = data.next_batch(FLAGS['batch_size'])\n",
    "            \n",
    "            if (index + 1) % FLAGS['train_report_step'] == 0:\n",
    "                _, loss_batch, summary, gs = sess.run([optimizer, loss, merged, global_step], \n",
    "                                    feed_dict={X: X_batch, Y:y_batch, dropout:FLAGS['keep_prob']}) \n",
    "                total_loss += loss_batch\n",
    "                average_loss = total_loss / gs\n",
    "                \n",
    "                \n",
    "                print('Average loss at step {}: {:5.1f}'.format(index + 1, average_loss))\n",
    "                train_writer.add_summary(summary, index)\n",
    "            else:\n",
    "                _, loss_batch, gs = sess.run([optimizer, loss, global_step], \n",
    "                                    feed_dict={X: X_batch, Y:y_batch, dropout:FLAGS['keep_prob']}) \n",
    "                total_loss += loss_batch\n",
    "                average_loss = total_loss / gs\n",
    "                if average_loss < best_loss:\n",
    "                    best_loss = average_loss\n",
    "            \n",
    "                \n",
    "            if (index + 1) % FLAGS['val_report_step'] == 0:\n",
    "                perm0 = np.arange(X_val.shape[0])\n",
    "                np.random.shuffle(perm0)\n",
    "                X_val= X_val[perm0]\n",
    "                y_val = y_val[perm0]\n",
    "                total_val_loss = 0\n",
    "                print('checking validation loss...')\n",
    "                for num in np.arange((X_val.shape[0] / FLAGS['batch_size']) - 1):\n",
    "                    start_num = num * 16\n",
    "                    end_num = int(start_num + 16)\n",
    "                    l_val_batch = sess.run([loss], \n",
    "                                           feed_dict={X: X_val[int(start_num):end_num, :,:,:], Y: y_val[int(start_num):end_num], dropout:1.0}) \n",
    "                    total_val_loss += l_val_batch[0]    \n",
    "                    \n",
    "                avg_val_loss = total_val_loss / (X_val.shape[0] - FLAGS['batch_size'])\n",
    "                print('validation loss: {}'.format(avg_val_loss))\n",
    "                if avg_val_loss > last_val_loss:\n",
    "                    print('validation loss increased')\n",
    "                    num_val_increases += 1 \n",
    "                    last_val_loss = avg_val_loss\n",
    "                    if num_val_increases > val_patience:\n",
    "                        print('Stopping training because validation loss has not decreased')\n",
    "                        print('trained for {} batches'.format(index))\n",
    "                        break\n",
    "                else:\n",
    "                    print('validation loss decreased...saving best params')\n",
    "                    last_val_loss = avg_val_loss\n",
    "                    num_val_increases = 0\n",
    "                    saver.save(sess, 'early_stopping_saver/', gs)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
